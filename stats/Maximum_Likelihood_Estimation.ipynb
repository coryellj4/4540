{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjvqhQuzyURH"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cyneuro/Neural-Networks-Machine-Learning/blob/main/stats/Maximum_Likelihood_Estimation.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "VqlbW1sWyURJ"
      },
      "source": [
        "## Maximum Likelihood Estimation\n",
        "\n",
        "This notebook assumes the data is a standard continuous normal distribution. The number of random samples, mean, and standard deviation of the distribution can be set in lines 7-9 in code cell 1.\n",
        "\n",
        "The function `gaussian` calculates the negative log likelihood given the random samples and the initial predicted mean and standard deviations defined in initParams.\n",
        "\n",
        "The key to the whole program is the scipy function `minimize`. It is a regular optimization function and more information about it can be found [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html).\n",
        "\n",
        "**Note:** the method can be a lot of different options, all different mathematical optimization techniques.\n",
        "\n",
        "\n",
        "#### Questions:\n",
        "1. If everything is working, we would expect more accurate $\\mu^*$ and $\\sigma^*$ predictions with more random samples. Think about why this is.\n",
        "2. What happens when the initial parameter guesses are extremely off? What about when they're extremely close? How does this impact the number of samples?\n",
        "3. Break down each line in the `gaussian` function. Knowing what you know about MLE, what do you think the function `stats.norm.logpdf` does?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "s4bzD2g5yURK",
        "outputId": "0b564ce5-93ae-49df-9ba1-f2c0008bea53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean*  = 97.928 std*  = 5.957\n",
            "mean   = 98.000 std   = 6.000\n"
          ]
        }
      ],
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "np.random.seed(1)\n",
        "\n",
        "\n",
        "samples = 10000\n",
        "mu      = 98\n",
        "sigma   = 6\n",
        "\n",
        "\n",
        "#  Take random sample from the distribution of data (normal dist in this case)\n",
        "sample_data = np.random.logistic(loc=mu, scale=sigma, size=samples)\n",
        "\n",
        "def gaussian(params):\n",
        "    mean = params[0]\n",
        "    sd = params[1]\n",
        "\n",
        "    # Calculate negative log likelihood\n",
        "    nll = -np.sum(stats.norm.logpdf(sample_data, loc=mean, scale=sd))\n",
        "\n",
        "    return nll\n",
        "\n",
        "\n",
        "def logistic(params):\n",
        "    mu_est = params[0]  # Estimated location parameter (mean)\n",
        "    s_est = params[1]   # Estimated scale parameter\n",
        "\n",
        "    # Calculate the negative log-likelihood for logistic distribution\n",
        "    nll = -np.sum(stats.logistic.logpdf(sample_data, loc=mu_est, scale=s_est))\n",
        "\n",
        "    return nll\n",
        "\n",
        "\n",
        "initParams = [1, 1]\n",
        "\n",
        "results = minimize(logistic, initParams, method='Nelder-Mead')\n",
        "\n",
        "\n",
        "print('mean*  = {:.3f} std*  = {:.3f}'.format(results.x[0],results.x[1]))\n",
        "print('mean   = {:.3f} std   = {:.3f}'.format(mu, sigma))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qTV38Kd9yURN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def normal_dist(inp, mean, std):\n",
        "    return 1./(np.sqrt(2.*np.pi)*std)*np.exp(-np.power((inp - mean)/std, 2.)/2)\n",
        "\n",
        "x = np.arange(-10, 10, 0.1)\n",
        "y = normal_dist(x, mu, sigma)\n",
        "\n",
        "num_bins = 20\n",
        "\n",
        "count, bins, ignored = plt.hist(sample_data, bins=num_bins, density=True)\n",
        "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2) ))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "tZfIyjWCyURN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NME",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}